{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. LangSmith を使った RAG アプリケーションの評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Ragas による合成テストデータの生成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パッケージのインストール\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索対象のドキュメントのロード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas による合成テストデータ生成の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    document.metadata[\"filename\"] = document.metadata[\"source\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【注意】既知のエラーについて\n",
    "\n",
    "以下のコードで gpt-4o を使用すると OpenAI API の Usage tier 次第で RateLimitError が発生することが報告されています。\n",
    "\n",
    "OpenAI API の Usage tier については公式ドキュメントの以下のページを参照してください。\n",
    "\n",
    "https://platform.openai.com/docs/guides/rate-limits/usage-tiers\n",
    "\n",
    "このエラーが発生した場合は、以下のどちらかの対応を実施してください。\n",
    "\n",
    "1. 同じ Tier でも gpt-4o よりレートリミットの高い gpt-4o-mini を使用する\n",
    "   - この場合、生成される合成テストデータの品質は低くなることが想定されます\n",
    "2. 課金などにより Tier を上げる\n",
    "   - Tier 2 で RateLimitError が発生しないことを確認済みです (2024 年 10 月 31 日時点)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471f38c67b8d421783f8b603b89c6551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b8cbbef3a84e67adb8a6fc1d869c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    critic_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents,\n",
    "    test_size=4,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key features and functionalities ...</td>\n",
       "      <td>[# MyScale\\n\\nThis page covers how to use MySc...</td>\n",
       "      <td>The key features and functionalities of the My...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'docs/docs/integrations/providers/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the steps involved in the installatio...</td>\n",
       "      <td>[# RWKV-4\\n\\nThis page covers how to use the `...</td>\n",
       "      <td>The steps involved in the installation and set...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'docs/docs/integrations/providers/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two key components for RWKV wrapper?</td>\n",
       "      <td>[# RWKV-4\\n\\nThis page covers how to use the `...</td>\n",
       "      <td>The two key components for the RWKV wrapper ar...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'docs/docs/integrations/providers/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does structured output improve chat model ...</td>\n",
       "      <td>[# Chat models\\n\\n## Overview\\n\\nLarge Languag...</td>\n",
       "      <td>Structured output improves chat model interact...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'docs/docs/concepts/chat_models.md...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the key features and functionalities ...   \n",
       "1  What are the steps involved in the installatio...   \n",
       "2  What are the two key components for RWKV wrapper?   \n",
       "3  How does structured output improve chat model ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [# MyScale\\n\\nThis page covers how to use MySc...   \n",
       "1  [# RWKV-4\\n\\nThis page covers how to use the `...   \n",
       "2  [# RWKV-4\\n\\nThis page covers how to use the `...   \n",
       "3  [# Chat models\\n\\n## Overview\\n\\nLarge Languag...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The key features and functionalities of the My...         simple   \n",
       "1  The steps involved in the installation and set...         simple   \n",
       "2  The two key components for the RWKV wrapper ar...      reasoning   \n",
       "3  Structured output improves chat model interact...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'docs/docs/integrations/providers/...          True  \n",
       "1  [{'source': 'docs/docs/integrations/providers/...          True  \n",
       "2  [{'source': 'docs/docs/integrations/providers/...          True  \n",
       "3  [{'source': 'docs/docs/concepts/chat_models.md...          True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith の Dataset の作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "dataset_name = \"agent-book\"\n",
    "\n",
    "client = Client()\n",
    "\n",
    "if client.has_dataset(dataset_name=dataset_name):\n",
    "    client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合成テストデータの保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What are the key features and functionalities of the MyScale vector database?\n",
      "contexts: ['# MyScale\\n\\nThis page covers how to use MyScale vector database within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific MyScale wrappers.\\n\\nWith MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale\\'s cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets.\\n\\n## Introduction\\n\\n[Overview to MyScale and High performance vector search](https://docs.myscale.com/en/overview/)\\n\\nYou can now register on our SaaS and [start a cluster now!](https://docs.myscale.com/en/quickstart/)\\n\\nIf you are also interested in how we managed to integrate SQL and vector, please refer to [this document](https://docs.myscale.com/en/vector-reference/) for further syntax reference.\\n\\nWe also deliver with live demo on huggingface! Please checkout our [huggingface space](https://huggingface.co/myscale)! They search millions of vector within a blink!\\n\\n## Installation and Setup\\n- Install the Python SDK with `pip install clickhouse-connect`\\n\\n### Setting up environments\\n\\nThere are two ways to set up parameters for myscale index.\\n\\n1. Environment Variables\\n\\n    Before you run the app, please set the environment variable with `export`:\\n    `export MYSCALE_HOST=\\'<your-endpoints-url>\\' MYSCALE_PORT=<your-endpoints-port> MYSCALE_USERNAME=<your-username> MYSCALE_PASSWORD=<your-password> ...`\\n\\n    You can easily find your account, password and other info on our SaaS. For details please refer to [this document](https://docs.myscale.com/en/cluster-management/)\\n    Every attributes under `MyScaleSettings` can be set with prefix `MYSCALE_` and is case insensitive.\\n\\n2. Create `MyScaleSettings` object with parameters\\n\\n\\n    ```python\\n    from langchain_community.vectorstores import MyScale, MyScaleSettings\\n    config = MyScaleSettings(host=\"<your-backend-url>\", port=8443, ...)\\n    index = MyScale(embedding_function, config)\\n    index.add_documents(...)\\n    ```\\n  \\n## Wrappers\\nsupported functions:\\n- `add_texts`\\n- `add_documents`\\n- `from_texts`\\n- `from_documents`\\n- `similarity_search`\\n- `asimilarity_search`\\n- `similarity_search_by_vector`\\n- `asimilarity_search_by_vector`\\n- `similarity_search_with_relevance_scores`\\n- `delete`\\n\\n### VectorStore\\n\\nThere exists a wrapper around MyScale database, allowing you to use it as a vectorstore,\\nwhether for semantic search or similar example retrieval.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import MyScale\\n```\\n\\nFor a more detailed walkthrough of the MyScale wrapper, see [this notebook](/docs/integrations/vectorstores/myscale)\\n']\n",
      "ground_truth: The key features and functionalities of the MyScale vector database include the ability to manage both structured and unstructured (vectorized) data, perform joint queries and analytics using SQL, and utilize a cloud-native OLAP architecture built on top of ClickHouse for fast data processing on massive datasets. It also supports various functions such as adding texts and documents, similarity searches, and provides a wrapper for use as a vectorstore.\n",
      "metadata: docs/docs/integrations/providers/myscale.mdx\n",
      "evolution_type: simple\n",
      "question: What are the steps involved in the installation and setup of the RWKV-4 wrapper within LangChain?\n",
      "contexts: ['# RWKV-4\\n\\nThis page covers how to use the `RWKV-4` wrapper within LangChain.\\nIt is broken into two parts: installation and setup, and then usage with an example.\\n\\n## Installation and Setup\\n- Install the Python package with `pip install rwkv`\\n- Install the tokenizer Python package with `pip install tokenizer`\\n- Download a [RWKV model](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) and place it in your desired directory\\n- Download the [tokens file](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20B_tokenizer.json)\\n\\n## Usage\\n\\n### RWKV\\n\\nTo use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer\\'s configuration.\\n```python\\nfrom langchain_community.llms import RWKV\\n\\n# Test the model\\n\\n```python\\n\\ndef generate_prompt(instruction, input=None):\\n    if input:\\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n# Instruction:\\n{instruction}\\n\\n# Input:\\n{input}\\n\\n# Response:\\n\"\"\"\\n    else:\\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n# Instruction:\\n{instruction}\\n\\n# Response:\\n\"\"\"\\n\\n\\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\\nresponse = model.invoke(generate_prompt(\"Once upon a time, \"))\\n```\\n## Model File\\n\\nYou can find links to model file downloads at the [RWKV-4-Raven](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) repository.\\n\\n### Rwkv-4 models -> recommended VRAM\\n\\n\\n```\\nRWKV VRAM\\nModel | 8bit | bf16/fp16 | fp32\\n14B   | 16GB | 28GB      | >50GB\\n7B    | 8GB  | 14GB      | 28GB\\n3B    | 2.8GB| 6GB       | 12GB\\n1b5   | 1.3GB| 3GB       | 6GB\\n```\\n\\nSee the [rwkv pip](https://pypi.org/project/rwkv/) page for more information about strategies, including streaming and cuda support.\\n']\n",
      "ground_truth: The steps involved in the installation and setup of the RWKV-4 wrapper within LangChain are: 1. Install the Python package with `pip install rwkv`. 2. Install the tokenizer Python package with `pip install tokenizer`. 3. Download a RWKV model and place it in your desired directory. 4. Download the tokens file.\n",
      "metadata: docs/docs/integrations/providers/rwkv.mdx\n",
      "evolution_type: simple\n",
      "question: What are the two key components for RWKV wrapper?\n",
      "contexts: ['# RWKV-4\\n\\nThis page covers how to use the `RWKV-4` wrapper within LangChain.\\nIt is broken into two parts: installation and setup, and then usage with an example.\\n\\n## Installation and Setup\\n- Install the Python package with `pip install rwkv`\\n- Install the tokenizer Python package with `pip install tokenizer`\\n- Download a [RWKV model](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) and place it in your desired directory\\n- Download the [tokens file](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20B_tokenizer.json)\\n\\n## Usage\\n\\n### RWKV\\n\\nTo use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer\\'s configuration.\\n```python\\nfrom langchain_community.llms import RWKV\\n\\n# Test the model\\n\\n```python\\n\\ndef generate_prompt(instruction, input=None):\\n    if input:\\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n# Instruction:\\n{instruction}\\n\\n# Input:\\n{input}\\n\\n# Response:\\n\"\"\"\\n    else:\\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n# Instruction:\\n{instruction}\\n\\n# Response:\\n\"\"\"\\n\\n\\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\\nresponse = model.invoke(generate_prompt(\"Once upon a time, \"))\\n```\\n## Model File\\n\\nYou can find links to model file downloads at the [RWKV-4-Raven](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) repository.\\n\\n### Rwkv-4 models -> recommended VRAM\\n\\n\\n```\\nRWKV VRAM\\nModel | 8bit | bf16/fp16 | fp32\\n14B   | 16GB | 28GB      | >50GB\\n7B    | 8GB  | 14GB      | 28GB\\n3B    | 2.8GB| 6GB       | 12GB\\n1b5   | 1.3GB| 3GB       | 6GB\\n```\\n\\nSee the [rwkv pip](https://pypi.org/project/rwkv/) page for more information about strategies, including streaming and cuda support.\\n']\n",
      "ground_truth: The two key components for the RWKV wrapper are the pre-trained model file and the tokenizer's configuration.\n",
      "metadata: docs/docs/integrations/providers/rwkv.mdx\n",
      "evolution_type: reasoning\n",
      "question: How does structured output improve chat model interactions vs. string-based models?\n",
      "contexts: ['# Chat models\\n\\n## Overview\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.\\n\\nThe newest generation of chat models offer additional capabilities:\\n\\n* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\n* [Structured output](/docs/concepts/structured_outputs): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n* [Multimodality](/docs/concepts/multimodality): The ability to work with data other than text; for example, images, audio, and video.\\n\\n## Features\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.\\n* Use either LangChain\\'s [messages](/docs/concepts/messages) format or OpenAI format.\\n* Standard [tool calling API](/docs/concepts/tool_calling): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\n* Standard API for [structuring outputs](/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\\n* Provides support for [async programming](/docs/concepts/async), [efficient batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](/docs/concepts/streaming).\\n* Integration with [LangSmith](https://docs.smith.langchain.com) for monitoring and debugging production-grade applications based on LLMs.\\n* Additional features like standardized [token usage](/docs/concepts/messages/#aimessage), [rate limiting](#rate-limiting), [caching](#caching) and more.\\n\\n## Integrations\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\n\\nThese integrations are one of two types:\\n\\n1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.\\n2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).\\n\\nPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.\\n\\n:::note\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\n:::\\n\\n\\n## Interface\\n\\nLangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](/docs/concepts/runnables), chat models support a [standard streaming interface](/docs/concepts/streaming), [async programming](/docs/concepts/async), optimized [', '# String-in, string-out llms\\n\\n:::tip\\nYou are probably looking for the [Chat Model Concept Guide](/docs/concepts/chat_models) page for more information.\\n:::\\n\\nLangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the \"Chat\" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.), and may include the \"LLM\" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface.\\n\\nUsers should be using almost exclusively the newer [Chat Models](/docs/concepts/chat_models) as most\\nmodel providers have adopted a chat like interface for interacting with language models.']\n",
      "ground_truth: Structured output improves chat model interactions by allowing models to respond in a structured format, such as JSON that matches a given schema, which enhances the clarity and usability of the output compared to string-based models that return unstructured text.\n",
      "metadata: docs/docs/concepts/chat_models.mdx\n",
      "evolution_type: multi_context\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "metadatas = []\n",
    "\n",
    "for testset_record in testset.test_data:\n",
    "    inputs.append(\n",
    "        {\n",
    "            \"question\": testset_record.question,\n",
    "        }\n",
    "    )\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"contexts\": testset_record.contexts,\n",
    "            \"ground_truth\": testset_record.ground_truth,\n",
    "        }\n",
    "    )\n",
    "    metadatas.append(\n",
    "        {\n",
    "            \"source\": testset_record.metadata[0][\"source\"],\n",
    "            \"evolution_type\": testset_record.evolution_type,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"question:\", testset_record.question)\n",
    "    print(\"contexts:\", testset_record.contexts)\n",
    "    print(\"ground_truth:\", testset_record.ground_truth)\n",
    "    print(\"metadata:\", testset_record.metadata[0][\"source\"])\n",
    "    print(\"evolution_type:\", testset_record.evolution_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    metadata=metadatas,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. LangSmith と Ragas を使ったオフライン評価の実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタム Evaluator の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langsmith.schemas import Example, Run\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM\n",
    "\n",
    "\n",
    "class RagasMetricEvaluator:\n",
    "    def __init__(self, metric: Metric, llm: BaseChatModel, embeddings: Embeddings):\n",
    "        self.metric = metric\n",
    "\n",
    "        # LLMとEmbeddingsをMetricに設定\n",
    "        if isinstance(self.metric, MetricWithLLM):\n",
    "            self.metric.llm = LangchainLLMWrapper(llm)\n",
    "        if isinstance(self.metric, MetricWithEmbeddings):\n",
    "            self.metric.embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "    def evaluate(self, run: Run, example: Example) -> dict[str, Any]:\n",
    "        context_strs = [doc.page_content for doc in run.outputs[\"contexts\"]]\n",
    "\n",
    "        # Ragasの評価メトリクスのscoreメソッドでスコアを算出\n",
    "        score = self.metric.score(\n",
    "            {\n",
    "                \"question\": example.inputs[\"question\"],\n",
    "                \"answer\": run.outputs[\"answer\"],\n",
    "                \"contexts\": context_strs,\n",
    "                \"ground_truth\": example.outputs[\"ground_truth\"],\n",
    "            },\n",
    "        )\n",
    "        return {\"key\": self.metric.name, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import answer_relevancy, context_precision\n",
    "\n",
    "metrics = [context_precision, answer_relevancy]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "evaluators = [\n",
    "    RagasMetricEvaluator(metric, llm, embeddings).evaluate\n",
    "    for metric in metrics\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論の関数の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "    question = inputs[\"question\"]\n",
    "    output = chain.invoke(question)\n",
    "    return {\n",
    "        \"contexts\": output[\"context\"],\n",
    "        \"answer\": output[\"answer\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オフライン評価の実装・実行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'virtual-limit-31' at:\n",
      "https://smith.langchain.com/o/0854a5ba-af85-42b8-900e-926caf3e92d9/datasets/14873920-3a2b-4c28-b56a-e63f4740456e/compare?selectedSessions=26669c61-d57d-4885-977b-d786f3388131\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ae934a53ce469ab6bfe73cb8b100e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.contexts</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.contexts</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>feedback.context_precision</th>\n",
       "      <th>feedback.answer_relevancy</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the two key components for RWKV wrapper?</td>\n",
       "      <td>[page_content='# RWKV-4\\n\\nThis page covers ho...</td>\n",
       "      <td>The two key components for the RWKV wrapper ar...</td>\n",
       "      <td>None</td>\n",
       "      <td>[# RWKV-4\\n\\nThis page covers how to use the `...</td>\n",
       "      <td>The two key components for the RWKV wrapper ar...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974559</td>\n",
       "      <td>1.535270</td>\n",
       "      <td>09e017c8-eae9-408d-9c3a-ffc1db9e87b4</td>\n",
       "      <td>63b7433c-da19-46d1-b2a7-c9bbc8d4f634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the steps involved in the installatio...</td>\n",
       "      <td>[page_content='# RWKV-4\\n\\nThis page covers ho...</td>\n",
       "      <td>The steps involved in the installation and set...</td>\n",
       "      <td>None</td>\n",
       "      <td>[# RWKV-4\\n\\nThis page covers how to use the `...</td>\n",
       "      <td>The steps involved in the installation and set...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.916894</td>\n",
       "      <td>a9dfa4e2-4723-4ea1-8e03-d5aad185d32d</td>\n",
       "      <td>cf9a2ed8-c9ec-4545-b033-4ca687075737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the key features and functionalities ...</td>\n",
       "      <td>[page_content='# MyScale\\n\\nThis page covers h...</td>\n",
       "      <td>MyScale vector database offers several key fea...</td>\n",
       "      <td>None</td>\n",
       "      <td>[# MyScale\\n\\nThis page covers how to use MySc...</td>\n",
       "      <td>The key features and functionalities of the My...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995958</td>\n",
       "      <td>5.778261</td>\n",
       "      <td>53825c5a-e5e7-421a-bb83-b7a0debb2562</td>\n",
       "      <td>581a8df5-7e37-4d66-ab17-5dabf99b4e1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does structured output improve chat model ...</td>\n",
       "      <td>[page_content='# Structured outputs\\n\\n## Over...</td>\n",
       "      <td>Structured output improves chat model interact...</td>\n",
       "      <td>None</td>\n",
       "      <td>[# Chat models\\n\\n## Overview\\n\\nLarge Languag...</td>\n",
       "      <td>Structured output improves chat model interact...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.997677</td>\n",
       "      <td>7.126468</td>\n",
       "      <td>aa4b19b3-12c2-469d-bf51-943c27a29e15</td>\n",
       "      <td>b6b18063-28f8-4707-855f-18b1b9d540bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults virtual-limit-31>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "evaluate(\n",
    "    predict,\n",
    "    data=\"agent-book\",\n",
    "    evaluators=evaluators,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith を使ったオンライン評価の実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィードバックボタンを表示する関数の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from langsmith import Client\n",
    "\n",
    "\n",
    "def display_feedback_buttons(run_id: UUID) -> None:\n",
    "    # GoodボタンとBadボタンを準備\n",
    "    good_button = widgets.Button(\n",
    "        description=\"Good\",\n",
    "        button_style=\"success\",\n",
    "        icon=\"thumbs-up\",\n",
    "    )\n",
    "    bad_button = widgets.Button(\n",
    "        description=\"Bad\",\n",
    "        button_style=\"danger\",\n",
    "        icon=\"thumbs-down\",\n",
    "    )\n",
    "\n",
    "    # クリックされた際に実行される関数を定義\n",
    "    def on_button_clicked(button: widgets.Button) -> None:\n",
    "        if button == good_button:\n",
    "            score = 1\n",
    "        elif button == bad_button:\n",
    "            score = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown button: {button}\")\n",
    "\n",
    "        client = Client()\n",
    "        client.create_feedback(run_id=run_id, key=\"thumbs\", score=score)\n",
    "        print(\"フィードバックを送信しました\")\n",
    "\n",
    "    # ボタンがクリックされた際にon_button_clicked関数を実行\n",
    "    good_button.on_click(on_button_clicked)\n",
    "    bad_button.on_click(on_button_clicked)\n",
    "\n",
    "    # ボタンを表示\n",
    "    display(good_button, bad_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィードバックボタンを表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\n",
      "\n",
      "1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\n",
      "\n",
      "2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\n",
      "\n",
      "3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\n",
      "\n",
      "LangChainは、LLMや関連技術（埋め込みモデルやベクターストアなど）に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、ユーザーは必要に応じてコンポーネントを選択して使用できます。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41eca082ddd54b878c5b74cf167bcf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Good', icon='thumbs-up', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658cd6704ad34faeb90bccb180399543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Bad', icon='thumbs-down', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フィードバックを送信しました\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tracers.context import collect_runs\n",
    "\n",
    "# LangSmithのトレースのID(Run ID)を取得するため、collect_runs関数を使用\n",
    "with collect_runs() as runs_cb:\n",
    "    output = chain.invoke(\"LangChainの概要を教えて\")\n",
    "    print(output[\"answer\"])\n",
    "    run_id = runs_cb.traced_runs[0].id\n",
    "\n",
    "display_feedback_buttons(run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
