{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Advanced RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. ãƒãƒ³ã‚ºã‚ªãƒ³ã®æº–å‚™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’åˆ©ç”¨ã—ã¦ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€LLMã‚„é–¢é€£æŠ€è¡“ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãªã©ï¼‰ã«å¯¾ã™ã‚‹æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã€æ•°ç™¾ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€è¤‡æ•°ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é¸æŠã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "ä»¥ä¸‹ã®æ–‡è„ˆã ã‘ã‚’è¸ã¾ãˆã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ–‡è„ˆ: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "è³ªå•: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. æ¤œç´¢ã‚¯ã‚¨ãƒªã®å·¥å¤«\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDEï¼ˆHypothetical Document Embeddingsï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "æ¬¡ã®è³ªå•ã«å›ç­”ã™ã‚‹ä¸€æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "è³ªå•: {question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚LangChainã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’åˆ©ç”¨ã—ã¦ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€ã•ã¾ã–ã¾ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã§ãã‚‹æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€é–‹ç™ºè€…ã¯ç•°ãªã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç°¡å˜ã«çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¯èƒ½ã«ãªã‚Šã€çŠ¶æ…‹ã®ä¿æŒã‚„äººé–“ã®ä»‹å…¥ã‚’å«ã‚€æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚LangSmithã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚„è©•ä¾¡ã‚’è¡Œã†ãŸã‚ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚\\n\\nå…¨ä½“ã¨ã—ã¦ã€LangChainã¯AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã‚’åŠ¹ç‡åŒ–ã—ã€é–‹ç™ºè€…ãŒè‡ªä¿¡ã‚’æŒã£ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hypothetical_chain | retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChainã¨ã¯ä½•ã‹ã€ãã®åŸºæœ¬çš„ãªæ©Ÿèƒ½ã¨ç›®çš„ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',\n",
       " 'LangChainã®ä¸»ãªåˆ©ç‚¹ã¨ä½¿ç”¨ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚',\n",
       " 'LangChainã‚’åˆ©ç”¨ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã‚„è€ƒæ…®ã™ã¹ãç‚¹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "è³ªå•ã«å¯¾ã—ã¦ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã«ã€\n",
    "3ã¤ã®ç•°ãªã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§æ¤œç´¢ã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€\n",
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦è¤‡æ•°ã®è¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚\n",
    "\n",
    "è³ªå•: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt | model.with_structured_output(QueryGenerationOutput) | (lambda x: x.queries)\n",
    ")\n",
    "\n",
    "query_generation_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚LangChainã¯ã€é–‹ç™ºã€é‹ç”¨ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’åˆ©ç”¨ã—ã¦ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **é‹ç”¨**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«ã‚„é–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã€é–‹ç™ºè€…ãŒç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–“ã§ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆãŸã‚Šã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ„ã¿åˆã‚ã›ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã¾ãŸã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¤‡é›‘ã•ãŒå¢—ã™ã«ã¤ã‚Œã¦ã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚„å¯è¦–æ€§ã€è©•ä¾¡ã®ãƒ‹ãƒ¼ã‚ºã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é–‹ç™ºè€…ã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å‹•ä½œã‚’ç†è§£ã—ã‚„ã™ãã—ã€è¿…é€Ÿã«è©•ä¾¡ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "multi_query_rag_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. æ¤œç´¢å¾Œã®å·¥å¤«\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical features for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'),\n",
       "  Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_type': '.mdx', 'source': 'docs/docs/introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain youâ€™ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ğŸ¦œğŸ› ï¸ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ğŸ¦œğŸ•¸ï¸ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'),\n",
       "  Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method`\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'),\n",
       "  Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/index.mdx'}, page_content='# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples â€” those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n')],\n",
       " [Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical features for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'),\n",
       "  Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method`\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'),\n",
       "  Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_type': '.mdx', 'source': 'docs/docs/introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain youâ€™ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ğŸ¦œğŸ› ï¸ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ğŸ¦œğŸ•¸ï¸ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'),\n",
       "  Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_type': '.mdx', 'source': 'docs/docs/tutorials/index.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n# Tutorials\\n\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\n\\n## Get started\\n\\nFamiliarize yourself with LangChain's open-source components by building simple applications.\\n\\nIf you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).\\n\\n- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).\\n- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\\n- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).\\n- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).\\n\\nRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.\\n\\n## Orchestration\\n\\nGet started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.\\n\\n- [Chatbots](/docs/tutorials/chatbot): Build a chatbot that incorporates memory.\\n- [Agents](/docs/tutorials/agents): Build an agent that interacts with external tools.\\n- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag): Build an application that uses your own documents to inform its responses.\\n- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\n- [Question-Answering with SQL](/docs/tutorials/sql_qa): Build a question-answering system that executes SQL queries to inform its responses.\\n- [Summarization](/docs/tutorials/summarization): Generate summaries of (potentially long) texts.\\n- [Question-Answering with Graph Databases](/docs/tutorials/graph): Build a question-answering system that queries a graph database to inform its responses.\\n\\n## LangSmith\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/tutorials/).\\n\\n### Evaluation\\n\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\\n\")],\n",
       " [Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_type': '.mdx', 'source': 'docs/docs/introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions.\\nThese how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain youâ€™ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [ğŸ¦œğŸ› ï¸ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [ğŸ¦œğŸ•¸ï¸ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'),\n",
       "  Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical features for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'),\n",
       "  Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/index.mdx'}, page_content='# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples â€” those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts\\n\\n- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\n## Glossary\\n\\n- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain\\'s representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI\\'s message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\n'),\n",
       "  Document(metadata={'file_name': 'index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_type': '.mdx', 'source': 'docs/docs/tutorials/index.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n# Tutorials\\n\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\n\\n## Get started\\n\\nFamiliarize yourself with LangChain's open-source components by building simple applications.\\n\\nIf you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).\\n\\n- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).\\n- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\\n- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).\\n- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).\\n\\nRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.\\n\\n## Orchestration\\n\\nGet started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.\\n\\n- [Chatbots](/docs/tutorials/chatbot): Build a chatbot that incorporates memory.\\n- [Agents](/docs/tutorials/agents): Build an agent that interacts with external tools.\\n- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag): Build an application that uses your own documents to inform its responses.\\n- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\n- [Question-Answering with SQL](/docs/tutorials/sql_qa): Build a question-answering system that executes SQL queries to inform its responses.\\n- [Summarization](/docs/tutorials/summarization): Generate summaries of (potentially long) texts.\\n- [Question-Answering with Graph Databases](/docs/tutorials/graph): Build a question-answering system that queries a graph database to inform its responses.\\n\\n## LangSmith\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/tutorials/).\\n\\n### Evaluation\\n\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\\n\")]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ (æ–‡å­—åˆ—) ã¨ãã®ã‚¹ã‚³ã‚¢ã®å¯¾å¿œã‚’ä¿æŒã™ã‚‹è¾æ›¸ã‚’æº–å‚™\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # æ¤œç´¢ã‚¯ã‚¨ãƒªã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "    for docs in retriever_outputs:\n",
    "        # æ¤œç´¢çµæœã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "\n",
    "            # åˆã‚ã¦ç™»å ´ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å ´åˆã¯ã‚¹ã‚³ã‚¢ã‚’0ã§åˆæœŸåŒ–\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1 / (é †ä½ + k)) ã®ã‚¹ã‚³ã‚¢ã‚’åŠ ç®—\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # ã‚¹ã‚³ã‚¢ã®å¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]\n",
    "\n",
    "\n",
    "a = query_generation_chain | retriever.map()\n",
    "a.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚LangChainã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’åˆ©ç”¨ã—ã¦ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚„åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãªã©ã®æŠ€è¡“ã«å¯¾ã—ã¦æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã€æ•°ç™¾ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é–‹ç™ºè€…ã¯ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆãŸã‚Šã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ„ã¿åˆã‚ã›ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nå…¨ä½“ã¨ã—ã¦ã€LangChainã¯AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã‚’åŠ¹ç‡åŒ–ã—ã€é–‹ç™ºè€…ãŒè¤‡é›‘ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã®éšœå£ã‚’ä½ãã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere ã®ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹æº–å‚™\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere ã®ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å°å…¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚LangGraphã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€ã•ã¾ã–ã¾ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã—ã€æ¨™æº–åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€é–‹ç™ºè€…ãŒç•°ãªã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆãŸã‚Šã€çµ„ã¿åˆã‚ã›ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€LangSmithã‚’é€šã˜ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¯è¦³æ¸¬æ€§ã¨è©•ä¾¡ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    documents = inp[\"documents\"]\n",
    "\n",
    "    cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=top_n)\n",
    "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
    "\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"documents\": retriever,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=rerank)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rerank_rag_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. è¤‡æ•°ã® Retriever ã‚’ä½¿ã†å·¥å¤«\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM ã«ã‚ˆã‚‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "langchain_document_retriever = retriever.with_config({\"run_name\": \"langchain_document_retriever\"})\n",
    "\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config({\"run_name\": \"web_retriever\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route.langchain_document\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«é©åˆ‡ãªRetrieverã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "è³ªå•: {question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = route_prompt | model.with_structured_output(RouteOutput) | (lambda x: x.route)\n",
    "\n",
    "route = route_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n",
    "print(route)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown retriever: {retriever}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚ã¾ãŸã€LangGraphã‚’åˆ©ç”¨ã—ã¦ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€LLMã‚„é–¢é€£æŠ€è¡“ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãªã©ï¼‰ã«å¯¾ã™ã‚‹æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€æ•°ç™¾ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€è¤‡æ•°ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®ãƒ‹ãƒ¼ã‚ºã«å¿œã˜ã¦ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é¸æŠã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ±äº¬ã®ä»Šæ—¥ã€2024å¹´11æœˆ19æ—¥(ç«)ã®å¤©æ°—ã¯ã€Œæ™´æ™‚ã€…æ›‡ã€ã§ã€æœ€é«˜æ°—æ¸©ã¯13â„ƒã€æœ€ä½æ°—æ¸©ã¯8â„ƒã§ã™ã€‚é™æ°´ç¢ºç‡ã¯0ï¼…ã§ã€åŒ—ã®é¢¨ãŒå¹ãäºˆå ±ã§ã™ã€‚'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"æ±äº¬ã®ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè£…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever = retriever.with_config({\"run_name\": \"chroma_retriever\"})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config({\"run_name\": \"bm25_retriever\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"chroma_documents\": chroma_retriever,\n",
    "            \"bm25_documents\": bm25_retriever,\n",
    "        }\n",
    "    )\n",
    "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    | reciprocal_rank_fusion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å„æ®µéšã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚\\n\\n1. **é–‹ç™º**: LangChainã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚LangGraphã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚„äººé–“ã®ä»‹å…¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n2. **ç”Ÿç”£åŒ–**: LangSmithã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œæŸ»ã€ç›£è¦–ã€è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«æœ€é©åŒ–ã—ã¦è‡ªä¿¡ã‚’æŒã£ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚\\n\\n3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿç”£æº–å‚™ãŒæ•´ã£ãŸAPIã‚„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤‰æ›ã§ãã¾ã™ã€‚\\n\\nLangChainã¯ã€LLMã‚„é–¢é€£æŠ€è¡“ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãªã©ï¼‰ã«å¯¾ã™ã‚‹æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã€æ•°ç™¾ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨çµ±åˆã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€è¤‡æ•°ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ç‰¹å®šã®ãƒ‹ãƒ¼ã‚ºã«å¿œã˜ã¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é¸æŠã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
